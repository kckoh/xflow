# XFlow - Modern Data Platform

**ì„¤ì • ê¸°ë°˜ì˜ ì‹œê°ì  ë°ì´í„° íŒŒì´í”„ë¼ì¸ í”Œë«í¼**

ì½”ë“œ ì—†ì´ ì›¹ UIì—ì„œ ETL/ELT íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„° í”Œë«í¼ì…ë‹ˆë‹¤.

<div align="center">

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green.svg)
![React](https://img.shields.io/badge/React-18.3-blue.svg)
![Spark](https://img.shields.io/badge/Spark-3.5-orange.svg)

</div>

---

## ì£¼ìš” ê¸°ëŠ¥

- **ë¹„ì£¼ì–¼ ETL ì—ë””í„°**: ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì„±
- **ë‹¤ì¤‘ ì†ŒìŠ¤ ì§€ì›**: PostgreSQL, MySQL, MongoDB, S3, REST API
- **ì‹¤ì‹œê°„ & ë°°ì¹˜**: CDC(Change Data Capture) + ë°°ì¹˜ ETL
- **ë°ì´í„° ì¹´íƒˆë¡œê·¸**: ìë™í™”ëœ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ë° ê²€ìƒ‰
- **ë°ì´í„° í’ˆì§ˆ**: ìë™ í’ˆì§ˆ ì²´í¬ ë° ëª¨ë‹ˆí„°ë§
- **AI ì¿¼ë¦¬ ì–´ì‹œìŠ¤í„´íŠ¸**: ìì—°ì–´ â†’ SQL ë³€í™˜ (AWS Bedrock)
- **ë¦¬ë‹ˆì§€ ì¶”ì **: ì†ŒìŠ¤ë¶€í„° íƒ€ê²Ÿê¹Œì§€ ë°ì´í„° íë¦„ ì‹œê°í™”
- **SQL Lab**: DuckDB/Trino ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ì¿¼ë¦¬ ì‹¤í–‰
- **Kubernetes ì§€ì›**: Production-ready K8s ë°°í¬ ì„¤ì •

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FRONTEND (React + Vite)                          â”‚
â”‚   Landing â”‚ Dataset â”‚ Catalog â”‚ Query â”‚ Quality â”‚ Admin     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ REST API
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BACKEND API (FastAPI)                                   â”‚
â”‚   27 Routers â”‚ 17 Services â”‚ MongoDB ODM â”‚ PostgreSQL ORM            â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€--
     â”‚      â”‚          â”‚          â”‚          
     â–¼      â–¼          â–¼          â–¼          
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
â”‚Airflow â”‚ â”‚    â”‚OpenSearchâ”‚ â”‚  Trino   â”‚ 
â”‚  DAG   â”‚ â”‚    â”‚(Search)  â”‚ â”‚ (Query)  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 
     â”‚     â”‚
     â–¼     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark       â”‚
â”‚ ETL Runner  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼                    â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚PostgreSQLâ”‚      â”‚  S3      â”‚   â”‚ MongoDB  â”‚
â”‚(Source/  â”‚      â”‚ (Data    â”‚   â”‚(Metadata)â”‚
â”‚ Dest)    â”‚      â”‚  Lake)   â”‚   â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Local Development (Docker Compose)

```bash
# 1. Clone repository
git clone https://github.com/yourusername/xflow.git
cd xflow

# 2. Start all services (15+ containers)
docker compose up -d

# 3. Download Spark JAR dependencies
mkdir -p spark/jars
curl -fsSLO --output-dir spark/jars https://jdbc.postgresql.org/download/postgresql-42.7.4.jar
curl -fsSLO --output-dir spark/jars https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar
curl -fsSLO --output-dir spark/jars https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar

# 4. Load sample data
docker compose exec -T postgres psql -U postgres -d mydb < init-fake-data.sql

# 5. Access the platform
open http://localhost:5173  # Frontend
```

## Services

| Service | URL | Credentials | Purpose |
|---------|-----|-------------|---------|
| **Frontend** | http://localhost:5173 | - | React ì›¹ UI |
| **Backend API** | http://localhost:8000 | - | FastAPI ì„œë²„ |
| **API Docs** | http://localhost:8000/docs | - | Swagger UI |
| **Airflow** | http://localhost:8080 | admin/admin | DAG ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ |
| **Spark Master UI** | http://localhost:8081 | - | Spark í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§ |
| **OpenSearch** | http://localhost:9200 | - | ì „ë¬¸ ê²€ìƒ‰ ì—”ì§„ |
| **OpenSearch Dashboards** | http://localhost:5601 | - | ë°ì´í„° ì‹œê°í™” |
| **Trino** | http://localhost:8082 | - | ë¶„ì‚° SQL ì¿¼ë¦¬ |
| **Kafka UI** | http://localhost:8084 | - | Kafka í† í”½ ê´€ë¦¬ |
| **PostgreSQL** | localhost:5433 | postgres/postgres | ê´€ê³„í˜• DB |
| **MongoDB** | localhost:27017 | - | ë¬¸ì„œí˜• DB (ë©”íƒ€ë°ì´í„°) |



## Core Features

### 1. Visual ETL Pipeline Builder

ì›¹ UIì—ì„œ ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ì„¸ìš”.

```
[Source Node] â†’ [Transform Node] â†’ [Target Node]
     â†“                 â†“                  â†“
  PostgreSQL      Filter/Select        S3 Parquet
  MongoDB         SQL Transform        Delta Lake
  S3 Files        Union/Join           PostgreSQL
  REST API        Custom Logic         MongoDB
```

**ì§€ì›í•˜ëŠ” ë³€í™˜ íƒ€ì… (10+):**
- `select-fields`: íŠ¹ì • ì»¬ëŸ¼ë§Œ ì„ íƒ
- `drop-columns`: ì»¬ëŸ¼ ì œê±°
- `filter`: SQL í‘œí˜„ì‹ í•„í„°ë§
- `union`: ë‹¤ì¤‘ ì…ë ¥ ë³‘í•©
- `sql`: ì„ì˜ì˜ Spark SQL ì‹¤í–‰
- `s3-select-fields`: S3 ë¡œê·¸ í•„ë“œ ì¶”ì¶œ
- `s3-filter`: S3 ë¡œê·¸ í•„í„°ë§

### 2. Data Catalog & Discovery

- **ìë™ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘**: ìŠ¤í‚¤ë§ˆ, í†µê³„, ìƒ˜í”Œ ë°ì´í„°
- **ì „ë¬¸ ê²€ìƒ‰**: OpenSearch ê¸°ë°˜ (í•œêµ­ì–´ Nori ë¶„ì„ê¸°)
- **ë„ë©”ì¸ ì¡°ì§í™”**: ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ë³„ ë°ì´í„° ìì‚° ê·¸ë£¹í™”
- **ë¦¬ë‹ˆì§€ ì‹œê°í™”**: ì†ŒìŠ¤ â†’ ë³€í™˜ â†’ íƒ€ê²Ÿ ë°ì´í„° íë¦„

### 3. Data Quality

DuckDB ê¸°ë°˜ Parquet íŒŒì¼ í’ˆì§ˆ ì²´í¬:
- Row count validation
- Null value detection
- Duplicate check
- Data quality scoring

### 4. AI-Powered Query Assistant

AWS Bedrock í†µí•©:
```
ìì—°ì–´: "ì§€ë‚œë‹¬ ë§¤ì¶œì´ ê°€ì¥ ë†’ì€ ìƒí’ˆ 10ê°œë¥¼ ë³´ì—¬ì¤˜"
   â†“
SQL: SELECT product_name, SUM(sales) as total_sales
     FROM products WHERE date >= '2026-01-01'
     GROUP BY product_name ORDER BY total_sales DESC LIMIT 10
```

### 5. SQL Lab (Interactive Query)

- **DuckDB**: S3 Parquet íŒŒì¼ ì§ì ‘ ì¿¼ë¦¬ (ì„œë²„ë¦¬ìŠ¤)
- **Trino**: ë¶„ì‚° SQL ì—”ì§„ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
- **Query History**: ì‹¤í–‰ ì´ë ¥ ì €ì¥ ë° ì¬ì‚¬ìš©

### 6. Change Data Capture (CDC)

Debezium + Kafkaë¥¼ í†µí•œ ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”:
```
PostgreSQL (logical replication)
    â†“
Kafka Topics (per table)
    â†“
S3 Data Lake / Delta Lake
```

---

## API Usage Examples

### 1. Create Connection (ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡)

```bash
curl -X POST http://localhost:8000/api/connections/ \
  -H "Content-Type: application/json" \
  -d '{
    "name": "postgres-main",
    "description": "Main PostgreSQL database",
    "type": "postgres",
    "host": "postgres",
    "port": 5432,
    "database": "mydb",
    "username": "postgres",
    "password": "postgres"
  }'
```

### 2. Create Dataset (ETL íŒŒì´í”„ë¼ì¸)

```bash
curl -X POST http://localhost:8000/api/datasets/ \
  -H "Content-Type: application/json" \
  -d '{
    "name": "products_to_s3",
    "description": "Extract products, transform, save to S3",
    "nodes": [
      {
        "id": "source1",
        "type": "rdb-source",
        "config": {
          "connection_id": "<connection_id>",
          "table": "products"
        }
      },
      {
        "id": "transform1",
        "type": "filter",
        "config": {
          "expression": "price > 100"
        }
      },
      {
        "id": "target1",
        "type": "s3-target",
        "config": {
          "path": "s3a://xflow-data/products",
          "format": "parquet"
        }
      }
    ],
    "edges": [
      {"from": "source1", "to": "transform1"},
      {"from": "transform1", "to": "target1"}
    ]
  }'
```

### 3. Run Dataset (íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)

```bash
curl -X POST http://localhost:8000/api/datasets/<dataset_id>/run
```

### 4. Check Status (ì‹¤í–‰ ìƒíƒœ í™•ì¸)

```bash
# List all runs
curl http://localhost:8000/api/job-runs/

# Get specific run with logs
curl http://localhost:8000/api/job-runs/<run_id>
```

## Technology Stack

### Frontend
- **React 18.3** + Vite 6.0
- **Tailwind CSS 4.1**
- **React Router 7.10**
- **XYFlow** - DAG ì‹œê°í™”
- **Recharts** - ì°¨íŠ¸/ê·¸ë˜í”„
- 110+ custom components

### Backend
- **FastAPI 0.115** - REST API í”„ë ˆì„ì›Œí¬
- **Beanie 1.26** - MongoDB ODM
- **SQLAlchemy 2.0** - PostgreSQL ORM
- **SlowAPI** - Rate limiting
- **boto3** - AWS S3/IAM ì—°ë™

### Data Processing
- **Apache Spark 3.5** - ë¶„ì‚° ETL ì‹¤í–‰
- **Apache Airflow 2.x** - DAG ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- **DuckDB 1.1** - ì„œë²„ë¦¬ìŠ¤ SQL ì¿¼ë¦¬
- **Trino 435** - ë¶„ì‚° SQL ì—”ì§„

### Storage & Search
- **PostgreSQL** - ê´€ê³„í˜• ë°ì´í„°
- **MongoDB** - íŒŒì´í”„ë¼ì¸ ë©”íƒ€ë°ì´í„°
- **OpenSearch 2.11** - ì „ë¬¸ ê²€ìƒ‰ 
- **MinIO / S3** - ê°ì²´ ì €ì¥ì†Œ (ë°ì´í„° ë ˆì´í¬)

### Infrastructure
- **Docker** + **Docker Compose** - ë¡œì»¬ ê°œë°œ
- **Kubernetes** - í”„ë¡œë•ì…˜ ë°°í¬
- **Terraform** - Infrastructure as Code
- **Prometheus** + **Grafana** - ëª¨ë‹ˆí„°ë§

---

## API Endpoints (27 Routers)

### Core Pipeline
- `POST /api/datasets/` - ETL íŒŒì´í”„ë¼ì¸ ìƒì„±
- `GET /api/datasets/` - íŒŒì´í”„ë¼ì¸ ëª©ë¡
- `GET /api/datasets/{id}` - íŒŒì´í”„ë¼ì¸ ìƒì„¸
- `PUT /api/datasets/{id}` - íŒŒì´í”„ë¼ì¸ ìˆ˜ì •
- `DELETE /api/datasets/{id}` - íŒŒì´í”„ë¼ì¸ ì‚­ì œ
- `POST /api/datasets/{id}/run` - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

### Connections
- `POST /api/connections/` - ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
- `GET /api/connections/` - ì—°ê²° ëª©ë¡
- `POST /api/connections/{id}/test` - ì—°ê²° í…ŒìŠ¤íŠ¸
- `DELETE /api/connections/{id}` - ì—°ê²° ì‚­ì œ

### Job Runs
- `GET /api/job-runs/` - ì‹¤í–‰ ì´ë ¥
- `GET /api/job-runs/{id}` - ì‹¤í–‰ ìƒì„¸ (ë¡œê·¸ í¬í•¨)
- `POST /api/job-runs/{id}/cancel` - ì‹¤í–‰ ì·¨ì†Œ

### Data Catalog
- `GET /api/catalog/` - ë°ì´í„° ì¹´íƒˆë¡œê·¸ íƒìƒ‰
- `GET /api/catalog/{id}` - ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„°
- `GET /api/catalog/{id}/lineage` - ë°ì´í„° ë¦¬ë‹ˆì§€
- `POST /api/catalog/search` - ì „ë¬¸ ê²€ìƒ‰

### Domains
- `POST /api/domains/` - ë„ë©”ì¸ ìƒì„±
- `GET /api/domains/` - ë„ë©”ì¸ ëª©ë¡
- `PUT /api/domains/{id}` - ë„ë©”ì¸ ìˆ˜ì •

### Data Quality
- `GET /api/quality/` - í’ˆì§ˆ ì²´í¬ ê²°ê³¼
- `POST /api/quality/{dataset_id}/check` - í’ˆì§ˆ ì²´í¬ ì‹¤í–‰
- `GET /api/quality/{dataset_id}/score` - í’ˆì§ˆ ì ìˆ˜

### Query Execution
- `POST /api/duckdb/query` - DuckDB ì¿¼ë¦¬ ì‹¤í–‰
- `POST /api/trino/query` - Trino ì¿¼ë¦¬ ì‹¤í–‰
- `GET /api/query-history/` - ì¿¼ë¦¬ ì´ë ¥

### AI Assistant
- `POST /api/ai/text-to-sql` - ìì—°ì–´ â†’ SQL ë³€í™˜
- `POST /api/ai/query-explain` - SQL ì¿¼ë¦¬ ì„¤ëª…

### CDC & Streaming
- `POST /api/cdc/connectors/` - CDC ì»¤ë„¥í„° ìƒì„±
- `GET /api/cdc/connectors/` - ì»¤ë„¥í„° ëª©ë¡
- `POST /api/kafka-streaming/topics` - Kafka í† í”½ ê´€ë¦¬

### Schema Detection
- `POST /api/s3-csv/schema` - S3 CSV ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
- `POST /api/s3-parquet/schema` - S3 Parquet ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
- `POST /api/s3-json/schema` - S3 JSON ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ

### Admin
- `POST /api/admin/users/` - ì‚¬ìš©ì ìƒì„±
- `GET /api/admin/users/` - ì‚¬ìš©ì ëª©ë¡
- `PUT /api/admin/roles/{id}` - ê¶Œí•œ ê´€ë¦¬

---

## Development

### Backend Development

```bash
cd backend

# Install dependencies
pip install -r requirements.txt

# Run with hot reload
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# API docs available at:
# - http://localhost:8000/docs (Swagger UI)
# - http://localhost:8000/redoc (ReDoc)
```

### Frontend Development

```bash
cd frontend

# Install dependencies
npm install

# Run dev server (Vite)
npm run dev  # Runs on http://localhost:5173

# Build for production
npm run build

# Preview production build
npm run preview
```

### Database Migrations (Alembic)

```bash
cd backend

# Create new migration
alembic revision --autogenerate -m "add new table"

# Apply migrations
alembic upgrade head

# Rollback migration
alembic downgrade -1

# Verify
docker compose exec postgres psql -U postgres -d mydb -c "\dt"
```

### Manual Spark Job Submission

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --master 'local[*]' \
  --driver-memory 2g \
  --jars /opt/spark/jars/extra/postgresql-42.7.4.jar,\
/opt/spark/jars/extra/hadoop-aws-3.3.4.jar,\
/opt/spark/jars/extra/aws-java-sdk-bundle-1.12.262.jar \
  /opt/spark/jobs/etl_runner.py \
  --config '{"source": {"type": "rdb", ...}}'
```

---

## Project Structure

```
xflow/
â”œâ”€â”€ backend/                   # FastAPI application
â”‚   â”œâ”€â”€ routers/              # API route handlers (27 files)
â”‚   â”œâ”€â”€ services/             # Business logic (17 files)
â”‚   â”œâ”€â”€ models.py             # MongoDB models (Beanie)
â”‚   â”œâ”€â”€ database.py           # PostgreSQL config
â”‚   â”œâ”€â”€ main.py               # FastAPI app
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ frontend/                  # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/           # Page components (14)
â”‚   â”‚   â”œâ”€â”€ components/      # Reusable components (110+)
â”‚   â”‚   â”œâ”€â”€ services/        # API client
â”‚   â”‚   â”œâ”€â”€ context/         # React context
â”‚   â”‚   â””â”€â”€ hooks/           # Custom hooks
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ airflow/                   # Orchestration
â”‚   â”œâ”€â”€ dags/                 # DAG definitions
â”‚   â”‚   â”œâ”€â”€ dataset_dag.py
â”‚   â”‚   â””â”€â”€ etl_common.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ spark/                     # Data processing
â”‚   â”œâ”€â”€ jobs/                 # ETL scripts (14)
â”‚   â”‚   â””â”€â”€ etl_runner.py    # Main ETL engine (2,000+ lines)
â”‚   â”œâ”€â”€ jars/                 # JDBC/Hadoop JARs
â”‚   â””â”€â”€ Dockerfile.k8s
â”œâ”€â”€ k8s/                       # Kubernetes manifests
â”‚   â”œâ”€â”€ backend/              # Backend deployment
â”‚   â”œâ”€â”€ airflow/              # Airflow Helm values
â”‚   â”œâ”€â”€ spark/                # Spark Operator CRDs
â”‚   â”œâ”€â”€ opensearch/           # OpenSearch cluster
â”‚   â”œâ”€â”€ kafka/                # Kafka cluster
â”‚   â”œâ”€â”€ mongodb/              # MongoDB StatefulSet
â”‚   â”œâ”€â”€ redis/                # Redis cache
â”‚   â””â”€â”€ trino/                # Trino coordinator
â”œâ”€â”€ terraform/                 # Infrastructure as Code
â”‚   â””â”€â”€ localstack/           # LocalStack config (S3, IAM)
â”œâ”€â”€ docker-compose.yml         # Local development (15+ services)
â”œâ”€â”€ init-fake-data.sql         # Sample data
â””â”€â”€ README.md
```

---

## Troubleshooting

### Spark Job Failed (Exit Code 137 - OOM)

```bash
# Use local mode instead of cluster
--master 'local[*]'

# Increase driver memory
--driver-memory 2g

# Increase executor memory
--executor-memory 2g
```

### Airflow DAG Not Appearing

```bash
# Restart scheduler
docker compose restart airflow-scheduler

# Check DAG errors
docker compose logs airflow-scheduler

# Verify DAG file syntax
docker compose exec airflow-scheduler python -c "import sys; sys.path.append('/opt/airflow/dags'); import dataset_dag"
```

### MongoDB Connection Refused

```bash
# Check MongoDB status
docker compose ps mongodb

# Restart MongoDB
docker compose restart mongodb

# Check logs
docker compose logs mongodb
```

### LocalStack S3 Not Working

```bash
# Restart LocalStack
docker compose restart localstack

# Wait for healthy status
docker compose ps localstack

# Recreate buckets
aws --endpoint-url=http://localhost:4566 s3 mb s3://xflow-data
```

### OpenSearch Cluster Yellow/Red

```bash
# Check cluster health
curl http://localhost:9200/_cluster/health?pretty

# Restart OpenSearch
docker compose restart opensearch

# Check disk space
docker system df
```

### Kafka Broker Not Ready

```bash
# Restart Kafka cluster
docker compose restart kafka zookeeper

# Check broker logs
docker compose logs kafka --tail=100

# List topics
docker compose exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

---

## Deployment

### Frontend Deployment (S3 + CloudFront)

```bash
cd frontend

# Build production bundle
npm run build

# Upload to S3
aws s3 sync dist/ s3://xflows --delete

# Invalidate CloudFront cache
aws cloudfront create-invalidation \
  --distribution-id E3J1KK599NCLXA \
  --paths "/*"
```

### Kubernetes (AWS EKS) Deployment

#### Prerequisites

1. **EKS Cluster Setup**
```bash
# Install eksctl
brew install eksctl

# Create EKS cluster
eksctl create cluster \
  --name xflow-cluster \
  --region ap-northeast-2 \
  --nodegroup-name standard-workers \
  --node-type t3.xlarge \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 5 \
  --managed
```

2. **Install Required Tools**
```bash
# Helm
brew install helm

# kubectl
brew install kubectl

# AWS CLI
brew install awscli
```

#### ECR Login (Common)

```bash
aws ecr get-login-password --region ap-northeast-2 > /tmp/ecr_password.txt
docker login --username AWS \
  --password-stdin 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com < /tmp/ecr_password.txt
rm /tmp/ecr_password.txt
```

---

#### 1. Backend Deployment

```bash
cd backend

# Build & push image
docker build --platform linux/amd64 \
  -t 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com/xflow-backend:latest .
docker push 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com/xflow-backend:latest

# Deploy (first time)
kubectl apply -f k8s/backend/

# Update deployment
kubectl rollout restart deployment backend -n default

# Check status
kubectl rollout status deployment backend -n default
kubectl get pods -n default -l app=backend

# View logs
kubectl logs -n default -l app=backend --tail=100 -f
```

**Configuration Files:**
- `k8s/backend/deployment.yaml` - Deployment spec
- `k8s/backend/configmap.yaml` - Environment variables
- `k8s/backend/serviceaccount.yaml` - IRSA (IAM role for S3)
- `k8s/backend/service.yaml` - ClusterIP service

---

#### 2. Airflow Deployment

```bash
cd airflow

# Build & push image (includes DAGs)
docker build --platform linux/amd64 \
  -t 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com/xflow-airflow:latest .
docker push 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com/xflow-airflow:latest

# Install Airflow (first time)
helm repo add apache-airflow https://airflow.apache.org
helm upgrade --install airflow apache-airflow/airflow \
    --namespace airflow \
    --create-namespace \
    --version 1.15.0 \
    -f k8s/airflow/values.yaml \
    --wait

# Deploy Spark RBAC
kubectl apply -f k8s/airflow/spark-rbac.yaml

# Update DAGs (after changes)
kubectl rollout restart deployment -n airflow \
  airflow-scheduler \
  airflow-webserver \
  airflow-triggerer \
  airflow-dag-processor

# Check status
kubectl get pods -n airflow

# View logs
kubectl logs -n airflow -l component=scheduler --tail=100 -f
```

**Configuration Files:**
- `k8s/airflow/values.yaml` - Helm chart values
- `k8s/airflow/spark-rbac.yaml` - Spark Operator permissions
- `k8s/airflow/ingress.yaml` - ALB Ingress config

---

#### 3. Spark Deployment

```bash
cd spark

# Build & push image
docker build --platform linux/amd64 -f Dockerfile.k8s \
  -t 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com/xflow-spark:latest .
docker push 134059028370.dkr.ecr.ap-northeast-2.amazonaws.com/xflow-spark:latest

# Install Spark Operator (first time)
helm repo add spark-operator https://kubeflow.github.io/spark-operator
helm install spark-operator spark-operator/spark-operator \
    --namespace spark-operator \
    --create-namespace \
    --set webhook.enable=true

# Jobs are automatically created by Airflow DAGs
# Monitor jobs:
kubectl get sparkapplication -n spark-jobs
```

**Spark Job Management:**
```bash
# List running jobs
kubectl get sparkapplication -n spark-jobs

# View job details
kubectl describe sparkapplication <name> -n spark-jobs

# View driver logs
kubectl logs -f <driver-pod-name> -n spark-jobs

# Delete job
kubectl delete sparkapplication <name> -n spark-jobs

# Delete all completed jobs
kubectl delete sparkapplication --all -n spark-jobs --field-selector status.applicationState.state=COMPLETED
```

---

#### 4. OpenSearch Deployment

```bash
# Deploy OpenSearch cluster
kubectl apply -f k8s/opensearch/

# Check cluster health
kubectl get pods -n opensearch

# View logs
kubectl logs -n opensearch -l app=opensearch
```

---

#### 5. Monitoring & Logging

```bash
# Install Prometheus + Grafana
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace

# Access Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# Open http://localhost:3000 (admin/prom-operator)
```

---

#### Infrastructure Summary

**Deployed Services:**
```bash
kubectl get pods -n default          # Backend, MongoDB
kubectl get pods -n airflow          # Airflow (scheduler, webserver, workers)
kubectl get pods -n spark-jobs       # Spark applications
kubectl get pods -n opensearch       # OpenSearch cluster
kubectl get pods -n monitoring       # Prometheus, Grafana
```

---

## Use Cases

1. **E-Commerce Analytics**: Extract orders from PostgreSQL â†’ Transform â†’ Store in S3 Data Lake â†’ Query with Trino
2. **Data Lake Ingestion**: Schedule daily S3 log parsing â†’ Parquet conversion â†’ Catalog registration
3. **Cross-Database Joins**: Union data from multiple sources (PostgreSQL + MongoDB + S3) â†’ Single view
4. **Data Quality Monitoring**: Automated quality checks on every pipeline run â†’ Alerts on failures

---

## License

This project is licensed under the MIT License. See `LICENSE` file for details.

---

## Acknowledgments

- **Apache Airflow** - Workflow orchestration
- **Apache Spark** - Distributed data processing
- **FastAPI** - Modern Python web framework
- **React** + **XYFlow** - Interactive UI components
- **OpenSearch** - Full-text search engine
- **Trino** - Distributed SQL query engine

---

## Support

- **Documentation**: [GitHub Wiki](https://github.com/yourusername/xflow/wiki)
- **Issues**: [GitHub Issues](https://github.com/yourusername/xflow/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/xflow/discussions)

---

## Roadmap

- [ ] Real-time streaming transformations (Flink integration)
- [ ] ML model deployment pipeline
- [ ] Data version control (lakeFS integration)
- [ ] Advanced lineage visualization (Neo4j integration)
- [ ] Multi-tenancy support
- [ ] SaaS deployment option

---

<div align="center">

**Built with â¤ï¸ for the Data Engineering Community**

[â­ Star this repo](https://github.com/yourusername/xflow) | [ğŸ› Report Bug](https://github.com/yourusername/xflow/issues) | [ğŸ’¡ Request Feature](https://github.com/yourusername/xflow/issues)

</div>
